# define network
class Model(nn.Module):

    def __init__(self, dropout_rate=0.5, n_outputs = 2):
        super(Model, self).__init__()

        self.seq = nn.Sequential(
            nn.Conv2d(3, 6, kernel_size=6, stride=2),  # First convolutional layer
            nn.ReLU(),  # SoftPlus, ReLu
            nn.MaxPool2d(2),  # Max pooling layer
            # nn.Dropout(p=dropout_rate),  # Dropout layer

            nn.Conv2d(6, 6, kernel_size=3, stride=1),  # Second convolutional layer
            nn.ReLU(),  # SoftPlus, ReLu
            nn.MaxPool2d(2),  # Max pooling layer
            # nn.Dropout(p=dropout_rate),  # Dropout layer

            nn.Flatten(),
            nn.Linear(in_features=1584, out_features=1584),  # Adjust the in_features based on the output size of the previous layer
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),  # Dropout layer

            nn.Linear(in_features=1584, out_features=n_outputs),
        )

        # Apply weight initialization
        self.apply(self.initialize_weights)

    def initialize_weights(self, module):
        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
            init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity='leaky_relu')
            if module.bias is not None:
                init.constant_(module.bias, 0)


    def forward(self, x):
        # Get features before the flattening operation
        features = self.seq[:5](x)  # Slices the sequence up to the flattening layer
        latent_layer = features

        # Continue with the rest of the network
        logits = self.seq[5:](features)
        return logits, latent_layer